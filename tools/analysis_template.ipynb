{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1768d6ec",
   "metadata": {},
   "source": [
    "# Binance Price Tracking Data Analysis\n",
    "\n",
    "This notebook provides analysis of cryptocurrency price movements following alert triggers.\n",
    "\n",
    "## Dataset Overview\n",
    "- **Source**: Automated tracking of price movements for 1 hour after alerts\n",
    "- **Features**: Price data, volatility metrics, market behavior, timing features\n",
    "- **Use Cases**: Predictive modeling, pattern recognition, strategy optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c823e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our custom data science utilities\n",
    "from data_science_utils import TrackingDataProcessor\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aea0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and explore the dataset\n",
    "processor = TrackingDataProcessor()\n",
    "processor.print_dataset_summary()\n",
    "\n",
    "# Load analytics DataFrame\n",
    "df = processor.to_analytics_dataframe()\n",
    "print(f\"\\nDataFrame shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6888ea",
   "metadata": {},
   "source": [
    "## 1. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa176ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"Dataset Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nNumerical Features Summary:\")\n",
    "df.describe().round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65433b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of key metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Final change distribution\n",
    "axes[0,0].hist(df['final_change_percent'], bins=30, alpha=0.7, color='skyblue')\n",
    "axes[0,0].axvline(0, color='red', linestyle='--', alpha=0.7)\n",
    "axes[0,0].set_title('Distribution of Final Price Changes')\n",
    "axes[0,0].set_xlabel('Final Change (%)')\n",
    "axes[0,0].set_ylabel('Frequency')\n",
    "\n",
    "# Volatility distribution\n",
    "axes[0,1].hist(df['volatility_stdev'], bins=30, alpha=0.7, color='lightcoral')\n",
    "axes[0,1].set_title('Distribution of Volatility')\n",
    "axes[0,1].set_xlabel('Volatility (Standard Deviation %)')\n",
    "axes[0,1].set_ylabel('Frequency')\n",
    "\n",
    "# Alert direction vs final change\n",
    "for direction in df['alert_direction_name'].unique():\n",
    "    subset = df[df['alert_direction_name'] == direction]\n",
    "    axes[1,0].hist(subset['final_change_percent'], alpha=0.6, label=f'{direction.title()} alerts', bins=20)\n",
    "axes[1,0].axvline(0, color='red', linestyle='--', alpha=0.7)\n",
    "axes[1,0].set_title('Final Change by Alert Direction')\n",
    "axes[1,0].set_xlabel('Final Change (%)')\n",
    "axes[1,0].set_ylabel('Frequency')\n",
    "axes[1,0].legend()\n",
    "\n",
    "# Threshold vs performance\n",
    "axes[1,1].scatter(df['threshold_value'], df['final_change_percent'], alpha=0.6)\n",
    "axes[1,1].set_title('Alert Threshold vs Final Performance')\n",
    "axes[1,1].set_xlabel('Alert Threshold (%)')\n",
    "axes[1,1].set_ylabel('Final Change (%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256c13fc",
   "metadata": {},
   "source": [
    "## 2. Symbol Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b3ff8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance by symbol\n",
    "symbol_stats = df.groupby('symbol').agg({\n",
    "    'final_change_percent': ['mean', 'std', 'count'],\n",
    "    'volatility_stdev': 'mean',\n",
    "    'positive_move_ratio': 'mean'\n",
    "}).round(4)\n",
    "\n",
    "symbol_stats.columns = ['avg_change', 'change_std', 'alert_count', 'avg_volatility', 'positive_ratio']\n",
    "symbol_stats = symbol_stats[symbol_stats['alert_count'] >= 3]  # Filter symbols with at least 3 alerts\n",
    "symbol_stats = symbol_stats.sort_values('avg_change', ascending=False)\n",
    "\n",
    "print(\"Symbol Performance (min 3 alerts):\")\n",
    "print(symbol_stats.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a70311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top performing symbols\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Top 10 by average performance\n",
    "top_10 = symbol_stats.head(10)\n",
    "axes[0].barh(range(len(top_10)), top_10['avg_change'], color='green', alpha=0.7)\n",
    "axes[0].set_yticks(range(len(top_10)))\n",
    "axes[0].set_yticklabels(top_10.index)\n",
    "axes[0].set_xlabel('Average Final Change (%)')\n",
    "axes[0].set_title('Top 10 Performing Symbols')\n",
    "axes[0].axvline(0, color='red', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Volatility vs Performance scatter\n",
    "axes[1].scatter(symbol_stats['avg_volatility'], symbol_stats['avg_change'], \n",
    "               s=symbol_stats['alert_count']*10, alpha=0.6)\n",
    "axes[1].set_xlabel('Average Volatility (%)')\n",
    "axes[1].set_ylabel('Average Final Change (%)')\n",
    "axes[1].set_title('Volatility vs Performance (bubble size = alert count)')\n",
    "axes[1].axhline(0, color='red', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7daffd0",
   "metadata": {},
   "source": [
    "## 3. Temporal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770906e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance by time of day and day of week\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Hour of day analysis\n",
    "hourly_performance = df.groupby('session_start_hour')['final_change_percent'].agg(['mean', 'count'])\n",
    "hourly_performance = hourly_performance[hourly_performance['count'] >= 2]  # Min 2 alerts per hour\n",
    "\n",
    "axes[0].bar(hourly_performance.index, hourly_performance['mean'], alpha=0.7, color='lightblue')\n",
    "axes[0].axhline(0, color='red', linestyle='--', alpha=0.7)\n",
    "axes[0].set_xlabel('Hour of Day (UTC)')\n",
    "axes[0].set_ylabel('Average Final Change (%)')\n",
    "axes[0].set_title('Performance by Hour of Day')\n",
    "axes[0].set_xticks(range(0, 24, 2))\n",
    "\n",
    "# Day of week analysis\n",
    "weekday_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "weekly_performance = df.groupby('session_start_weekday')['final_change_percent'].agg(['mean', 'count'])\n",
    "\n",
    "axes[1].bar(range(7), weekly_performance['mean'], alpha=0.7, color='lightgreen')\n",
    "axes[1].axhline(0, color='red', linestyle='--', alpha=0.7)\n",
    "axes[1].set_xlabel('Day of Week')\n",
    "axes[1].set_ylabel('Average Final Change (%)')\n",
    "axes[1].set_title('Performance by Day of Week')\n",
    "axes[1].set_xticks(range(7))\n",
    "axes[1].set_xticklabels(weekday_names)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296ce799",
   "metadata": {},
   "source": [
    "## 4. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3ab858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix of key features\n",
    "correlation_features = [\n",
    "    'threshold_value', 'alert_direction', 'monitoring_window_minutes',\n",
    "    'final_change_percent', 'max_change_percent', 'min_change_percent',\n",
    "    'volatility_stdev', 'positive_move_ratio', 'session_start_hour',\n",
    "    'actual_duration_seconds', 'total_data_points'\n",
    "]\n",
    "\n",
    "available_features = [f for f in correlation_features if f in df.columns]\n",
    "corr_matrix = df[available_features].corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='RdBu_r', center=0, \n",
    "            square=True, fmt='.2f', cbar_kws={\"shrink\": .8})\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Key correlations with final performance\n",
    "final_change_corr = corr_matrix['final_change_percent'].abs().sort_values(ascending=False)\n",
    "print(\"\\nStrongest correlations with final_change_percent:\")\n",
    "print(final_change_corr.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabeb275",
   "metadata": {},
   "source": [
    "## 5. Time Series Analysis (Sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc468ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load time series data and analyze a few interesting sessions\n",
    "ts_df = processor.to_price_timeseries_dataframe()\n",
    "\n",
    "if not ts_df.empty:\n",
    "    # Find sessions with extreme performance\n",
    "    session_stats = df.groupby('session_id')['final_change_percent'].first().sort_values()\n",
    "    \n",
    "    # Plot best and worst performing sessions\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Best performer\n",
    "    best_session = session_stats.index[-1]\n",
    "    best_data = ts_df[ts_df['session_id'] == best_session]\n",
    "    if not best_data.empty:\n",
    "        axes[0,0].plot(best_data['normalized_time'], best_data['change_from_initial'])\n",
    "        axes[0,0].set_title(f'Best Performer: {best_data.iloc[0][\"symbol\"]} ({session_stats.iloc[-1]:.2f}%)')\n",
    "        axes[0,0].set_xlabel('Normalized Time (0=start, 1=end)')\n",
    "        axes[0,0].set_ylabel('Change from Initial (%)')\n",
    "        axes[0,0].axhline(0, color='red', linestyle='--', alpha=0.5)\n",
    "        axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Worst performer\n",
    "    worst_session = session_stats.index[0]\n",
    "    worst_data = ts_df[ts_df['session_id'] == worst_session]\n",
    "    if not worst_data.empty:\n",
    "        axes[0,1].plot(worst_data['normalized_time'], worst_data['change_from_initial'], color='red')\n",
    "        axes[0,1].set_title(f'Worst Performer: {worst_data.iloc[0][\"symbol\"]} ({session_stats.iloc[0]:.2f}%)')\n",
    "        axes[0,1].set_xlabel('Normalized Time (0=start, 1=end)')\n",
    "        axes[0,1].set_ylabel('Change from Initial (%)')\n",
    "        axes[0,1].axhline(0, color='red', linestyle='--', alpha=0.5)\n",
    "        axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Average trajectory for up vs down alerts\n",
    "    for idx, alert_type in enumerate(['up', 'down']):\n",
    "        alert_sessions = df[df['threshold_type'] == alert_type]['session_id'].tolist()\n",
    "        alert_ts_data = ts_df[ts_df['session_id'].isin(alert_sessions)]\n",
    "        \n",
    "        if not alert_ts_data.empty:\n",
    "            # Create time bins and average\n",
    "            time_bins = np.linspace(0, 1, 21)  # 20 intervals\n",
    "            binned_data = []\n",
    "            \n",
    "            for i in range(len(time_bins)-1):\n",
    "                mask = (alert_ts_data['normalized_time'] >= time_bins[i]) & \\\n",
    "                       (alert_ts_data['normalized_time'] < time_bins[i+1])\n",
    "                if mask.any():\n",
    "                    avg_change = alert_ts_data[mask]['change_from_initial'].mean()\n",
    "                    binned_data.append(avg_change)\n",
    "                else:\n",
    "                    binned_data.append(np.nan)\n",
    "            \n",
    "            time_centers = (time_bins[:-1] + time_bins[1:]) / 2\n",
    "            axes[1,idx].plot(time_centers, binned_data, marker='o', \n",
    "                           label=f'Average trajectory ({len(alert_sessions)} sessions)')\n",
    "            axes[1,idx].set_title(f'Average Trajectory: {alert_type.upper()} Alerts')\n",
    "            axes[1,idx].set_xlabel('Normalized Time')\n",
    "            axes[1,idx].set_ylabel('Average Change from Initial (%)')\n",
    "            axes[1,idx].axhline(0, color='red', linestyle='--', alpha=0.5)\n",
    "            axes[1,idx].grid(True, alpha=0.3)\n",
    "            axes[1,idx].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No time series data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9dd544",
   "metadata": {},
   "source": [
    "## 6. Machine Learning Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfd4c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Get feature matrix and target\n",
    "X, y = processor.get_feature_matrix_for_ml(target_column='final_change_percent')\n",
    "\n",
    "if len(X) > 10:  # Need at least 10 samples\n",
    "    print(f\"Feature matrix shape: {X.shape}\")\n",
    "    print(f\"Target vector shape: {y.shape}\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Train a simple model\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\nModel Performance:\")\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(f\"R¬≤ Score: {r2:.4f}\")\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_names = [\n",
    "        'start_hour', 'weekday', 'duration', 'threshold', 'alert_dir', 'window_min',\n",
    "        'initial_price', 'max_change', 'min_change', 'volatility', 'variance',\n",
    "        'avg_abs_change', 'positive_ratio', 'data_points', 'data_freq'\n",
    "    ][:X.shape[1]]  # Adjust to actual number of features\n",
    "    \n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 Most Important Features:\")\n",
    "    print(importance_df.head(10))\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(range(len(importance_df.head(10))), importance_df.head(10)['importance'])\n",
    "    plt.yticks(range(len(importance_df.head(10))), importance_df.head(10)['feature'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title('Top 10 Feature Importances for Predicting Final Change')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(f\"Insufficient data for ML analysis. Found {len(X)} samples, need at least 10.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a805ba46",
   "metadata": {},
   "source": [
    "## 7. Export Data for External Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a74b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export processed data to CSV files\n",
    "processor.export_to_csv(\"analysis_exports\")\n",
    "\n",
    "print(\"\\nüìÅ Data exported to 'analysis_exports/' directory\")\n",
    "print(\"Files created:\")\n",
    "print(\"- tracking_analytics.csv: Main analytics dataset\")\n",
    "print(\"- price_timeseries.csv: Detailed price movements\")\n",
    "print(\"- symbol_statistics.csv: Summary stats by symbol\")\n",
    "print(\"\\nThese files can be used in other tools like Excel, R, or other Python environments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9589c11f",
   "metadata": {},
   "source": [
    "## Conclusions and Next Steps\n",
    "\n",
    "This analysis provides insights into:\n",
    "1. **Symbol Performance**: Which cryptocurrencies perform better after alerts\n",
    "2. **Temporal Patterns**: Best times of day/week for alert effectiveness\n",
    "3. **Volatility Analysis**: Relationship between volatility and performance\n",
    "4. **Predictive Features**: Which factors are most important for predicting outcomes\n",
    "\n",
    "### Potential Extensions:\n",
    "- **Deep Learning**: LSTM models for time series prediction\n",
    "- **Clustering**: Group similar price movement patterns\n",
    "- **Anomaly Detection**: Identify unusual market behaviors\n",
    "- **Strategy Optimization**: Use ML to optimize alert thresholds\n",
    "- **Real-time Prediction**: Deploy models for live trading decisions"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
